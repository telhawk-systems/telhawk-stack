# CLAUDE.md

Guidance for code agents working in this repository. Keep commits clean and avoid AI attribution.

## Critical Instructions
- Do not add “Co-Authored-By/Generated by …” footers to commits.
- Prefer linking to existing docs over repeating content.
- Use `docs/README.md` as the navigation root.

## Project Overview

TelHawk Stack is an OCSF‑compatible SIEM in Go with Splunk‑compatible ingestion and OpenSearch storage. Services: ingest → storage ↔ OpenSearch → query → web.

## Development Commands

### Building and Testing (minimal)

```bash
# Build one service
cd <service> && go build -o ../bin/<service> ./cmd/<service>
# Run tests
go test ./...
```

### TelHawk CLI (thawk)

The `thawk` CLI provides command-line access to all TelHawk services. Use the smart wrapper script:

```bash
# Authentication
./scripts/thawk auth login -u admin -p admin123
./scripts/thawk auth whoami

# Detection rules
./scripts/thawk rules list
./scripts/thawk rules get <rule-id>
./scripts/thawk rules create rules/failed_logins.json

# Alerts and cases
./scripts/thawk alerts list
./scripts/thawk alerts cases list

# Event seeding
./scripts/thawk seeder run --from-rules ./alerting/rules/

# All commands
./scripts/thawk --help
```

**How it works:** The wrapper automatically builds thawk (if Go is available) or rebuilds the devtools image, then executes commands via the devtools container with proper network access and environment variables.

### Docker / Devtools

See `docs/LOCAL_DEVELOPMENT.md` and `docs/HELPER_SCRIPTS.md` for detailed usage.

### Internal Access & Scripts

Use the `./scripts/curl.sh` wrapper to access internal services from your host machine.

**Quick Start:**
```bash
# Access internal services that aren't exposed to the host
./scripts/curl.sh http://alerting:8085/api/v1/alerts?page=1&limit=5
./scripts/curl.sh http://rules:8084/api/v1/schemas
./scripts/curl.sh -s http://query:8082/health | jq '.'
```

**How it works:** The script runs curl inside a Docker container connected to the TelHawk network, allowing access to internal services (auth, rules, query, alerting, etc.) that are not exposed on localhost.

**Example script** (`tmp/create_detection_rules.sh`):
```bash
#!/bin/bash
# Create multiple detection rules

RULES_API="http://rules:8084/api/v1/schemas"

for severity in critical high medium; do
  curl -s -X POST "$RULES_API" \
    -H "Content-Type: application/json" \
    -d "{
      \"model\": {...},
      \"view\": {\"severity\": \"$severity\", ...},
      \"controller\": {...}
    }" | jq -r '.id'
done
```

**File locations:**
- Scripts in `./tmp/` are accessible at `/tmp/` in the container (read/write)
- Scripts in `./scripts/` are accessible at `/scripts/` in the container (read-only)

**How it works:** Both tools use an Alpine-based Docker image (`telhawk-stack-devtools`) with curl, bash, jq, and wget. The image is automatically built and connected to the TelHawk internal network.

### Database Migrations

The auth service uses golang-migrate for database schema management:

```bash
# Migrations are automatically run on auth service startup
# Migration files: auth/migrations/*.sql

# To manually run migrations:
cd auth
# View migration status
migrate -database "postgres://telhawk:password@localhost:5432/telhawk_auth?sslmode=disable" -path migrations version

# Apply migrations
migrate -database "postgres://telhawk:password@localhost:5432/telhawk_auth?sslmode=disable" -path migrations up

# Rollback last migration
migrate -database "postgres://telhawk:password@localhost:5432/telhawk_auth?sslmode=disable" -path migrations down 1
```

### Generating Test Data (Event Seeder)

The event seeder generates realistic OCSF events for development and testing. It now supports **rule-based event generation** that automatically creates events matching detection rules to validate they fire correctly.

**Key features:**
- **Rule-based generation**: Reads detection rules and generates events that match them
- **Automatic validation**: Ensures generated events will trigger the rules
- **Jittered distribution**: Events are evenly distributed across time with ±40% random jitter
- **Guaranteed coverage**: Requires minimum 1 event/day to prevent gaps
- **Homogeneous baseline**: Ideal for layering suspicious activity patterns on top

#### Quick Start (Recommended)

Use the automated setup script for first-time seeding:

```bash
# Run the first-time seeding script (handles everything automatically)
./scripts/first-time-seed.sh
```

**What this script does:**
1. ✓ Starts required Docker services (if not running)
2. ✓ Builds the CLI tool (if needed)
3. ✓ Authenticates with the auth service
4. ✓ Creates or validates HEC token (saved to `.hec-token` - git-ignored)
5. ✓ Runs seeder for all detection rules
6. ✓ Verifies events in OpenSearch

**Output example:**
```
[1/6] Checking Docker services...
✓ Services are already running

[2/6] Checking CLI build...
✓ CLI already built

[3/6] Authenticating with auth service...
✓ Authenticated as admin

[4/6] Managing HEC token...
✓ Existing token is valid

[5/6] Generating events from detection rules...
  Events sent: 241

[6/6] Verifying events in OpenSearch...
✓ Total events in OpenSearch: 385

✓ Seeding Complete!
HEC Token: 019a999c-0214-73b9-9d7c-fed42a796feb
Token saved to: .hec-token (git-ignored)
```

#### Manual Seeding (Advanced)

If you need fine-grained control:

```bash
# 1. Create a HEC token (via web UI at http://localhost:3000/tokens or via API):
curl -X POST http://localhost:3000/api/auth/login \
  -H "Content-Type: application/json" \
  -d '{"username":"admin","password":"admin123"}' \
  -c /tmp/cookies.txt

curl -b /tmp/cookies.txt -X POST http://localhost:3000/api/auth/api/v1/hec/tokens \
  -H "Content-Type: application/json" \
  -d '{"name":"Event Seeder"}'

# Note the token value from the response

# 2. Build the CLI (if not already built):
cd cli
go build -o ../bin/thawk .

# 3. Rule-based event generation:

# Generate events from detection rules directory (validates rules fire):
./bin/thawk seeder run --token YOUR_TOKEN --from-rules ./alerting/rules/

# List available rules and their support status:
./bin/thawk seeder list-rules ./alerting/rules/

# Use YAML config for fine-grained control:
cat > seeder.yaml <<EOF
version: "1.0"
defaults:
  hec_url: http://localhost:8088
  token: YOUR_TOKEN
  count: 1000
  time_spread: 1h
  batch_size: 50

rules:
  failed_login_attack:
    rule_file: ./alerting/rules/failed_logins.json
    enabled: true
    multiplier: 1.5  # Generate 50% more events than threshold
    params:
      actor.user.name: admin
      src_endpoint.ip: 192.168.1.100

  port_scan_attack:
    rule_file: ./alerting/rules/port_scanning.json
    enabled: true
    multiplier: 2.0
EOF

./bin/thawk seeder run --config seeder.yaml
```

**Rule-based generation details:**
- **Supported correlation types** (Phase 1):
  - `event_count`: Generates N events matching filter (e.g., failed logins)
  - `value_count`: Generates events with N unique values (e.g., port scanning)
- **Not yet supported** (Phase 2):
  - `temporal_ordered`: Event sequences with strict order
  - `temporal`: Multiple events in any order
  - `join`: Field correlation
- **Benefits**:
  - Validates detection rules actually work
  - Automatically stays in sync with rule changes
  - Includes MITRE ATT&CK mapping from rules
  - No manual coding of attack patterns needed

**Supported OCSF Fields (17 Core Fields):**

The seeder intelligently generates realistic values for these critical SIEM detection fields:

| Tier | Field | Usage | Generated Value Example |
|------|-------|-------|------------------------|
| **Universal** | `.class_uid` | Event type classification | 3002 (Authentication), 4001 (Network), 1007 (Process) |
| | `.status_id` | Success/Failure indicator | 1 (Success), 2 (Failure) |
| | `.time` | Event timestamp | Unix timestamp with jitter |
| | `.severity_id` | Event severity | 1-5 (Info to Critical) |
| **Identity** | `.actor.user.name` | Who performed action | "jsmith", "admin", "dbaker" |
| | `.actor.user.uid` | User identifier | UUID v4 |
| | `.user.name` | Target user | "root", "administrator" |
| **Network** | `.src_endpoint.ip` | Source IP address | "192.168.1.100" |
| | `.dst_endpoint.ip` | Destination IP | "10.0.0.50" |
| | `.dst_endpoint.port` | Destination port | 22, 443, 3389 (integer) |
| | `.src_endpoint.port` | Source port | 1024-65535 (integer) |
| **Device** | `.device.hostname` | Device name | "server-01.example.com" |
| | `.device.ip` | Device IP | "172.16.0.10" |
| **Process/File** | `.process.name` | Process name | "sshd", "powershell.exe", "bash" |
| | `.process.pid` | Process ID | 1234 (integer) |
| | `.process.cmd_line` | Command line | "/bin/bash -c 'whoami'" |
| | `.file.path` | File path | "/etc/passwd", "C:\\Windows\\System32\\config\\SAM" |

**Baseline Field Population:**

To create realistic events that match production data, the seeder automatically populates these optional fields even if not required by the rule:

- **Device context**: `device.hostname`, `device.ip`, `device.os.name`, `device.type`
- **Metadata**: `metadata.product.vendor_name`, `metadata.product.name`, `metadata.version`
- **Connection info** (network events): `connection_info.protocol_name`, `connection_info.direction`
- **Session info** (authentication): `actor.session.uid`, `actor.session.issuer`
- **Process context** (process events): `process.pid`, `process.cmd_line`, `process.file.path`

**Example Generated Events:**

<details>
<summary>Authentication Failure (class_uid: 3002)</summary>

```json
{
  "class_uid": 3002,
  "category_uid": 3,
  "category_name": "Identity & Access Management",
  "class_name": "Authentication",
  "activity_id": 1,
  "activity_name": "Logon",
  "status_id": 2,
  "status": "Failure",
  "severity_id": 3,
  "time": 1732000000,
  "actor": {
    "user": {
      "name": "admin",
      "uid": "a1b2c3d4-e5f6-7890-abcd-ef1234567890"
    },
    "session": {
      "uid": "sess-12345",
      "issuer": "TelHawk Auth",
      "created_at": 1731998000
    }
  },
  "src_endpoint": {
    "ip": "192.168.1.100",
    "port": 45678
  },
  "device": {
    "hostname": "web-server-01.corp.example.com",
    "ip": "10.0.1.50",
    "type": "Server",
    "type_id": 1,
    "os": {
      "name": "Linux",
      "type": "Linux",
      "type_id": 200
    }
  },
  "metadata": {
    "product": {
      "vendor_name": "TelHawk",
      "name": "Event Seeder",
      "version": "2.0.0-rule-based"
    },
    "version": "1.1.0"
  }
}
```
</details>

<details>
<summary>Port Scanning (class_uid: 4001)</summary>

```json
{
  "class_uid": 4001,
  "category_uid": 4,
  "category_name": "Network Activity",
  "class_name": "Network Activity",
  "activity_id": 5,
  "activity_name": "Traffic",
  "severity_id": 1,
  "time": 1732000010,
  "src_endpoint": {
    "ip": "192.168.1.100",
    "port": 54321,
    "name": "",
    "uid": ""
  },
  "dst_endpoint": {
    "ip": "10.0.0.50",
    "port": 22,
    "name": "",
    "uid": ""
  },
  "connection_info": {
    "protocol_name": "TCP",
    "direction": "outbound",
    "direction_id": 2
  },
  "device": {
    "hostname": "scanner-vm.example.com",
    "ip": "192.168.1.100",
    "type": "Server",
    "type_id": 1,
    "os": {
      "name": "Linux",
      "type": "Linux",
      "type_id": 200
    }
  },
  "metadata": {
    "product": {
      "vendor_name": "TelHawk",
      "name": "Event Seeder",
      "version": "2.0.0-rule-based"
    },
    "version": "1.1.0"
  }
}
```
</details>

<details>
<summary>Process Activity (class_uid: 1007)</summary>

```json
{
  "class_uid": 1007,
  "category_uid": 1,
  "category_name": "System Activity",
  "class_name": "Process Activity",
  "activity_id": 1,
  "activity_name": "Launch",
  "severity_id": 1,
  "time": 1732000020,
  "process": {
    "name": "bash",
    "pid": 12345,
    "cmd_line": "/bin/bash -c 'curl http://example.com/script.sh | bash'",
    "file": {
      "path": "/bin/bash",
      "name": "bash"
    }
  },
  "actor": {
    "user": {
      "name": "jsmith",
      "uid": "b2c3d4e5-f6a7-8901-bcde-f23456789012"
    }
  },
  "device": {
    "hostname": "workstation-05.corp.local",
    "ip": "172.16.0.25",
    "type": "Server",
    "type_id": 1,
    "os": {
      "name": "Linux",
      "type": "Linux",
      "type_id": 200
    }
  },
  "metadata": {
    "product": {
      "vendor_name": "TelHawk",
      "name": "Event Seeder",
      "version": "2.0.0-rule-based"
    },
    "version": "1.1.0"
  }
}
```
</details>

## Architecture
See `docs/SERVICES.md` for current service flow and summaries.

See `docs/ARCHITECTURE_V2.md` for the proposed architecture evolution including:
- Service consolidation (7 → 5 services)
- NATS message broker integration
- Storage and auth boundaries
- Migration path

### Supporting Services

- OpenSearch (9200, 9600)
- PostgreSQL for auth/rules where applicable
- Redis for rate limiting/caching where applicable

## Code Architecture

### Event Pipeline (Ingest → Storage)

1. **Ingest Service** receives raw events via HEC endpoint (`/services/collector/event`)
   - Validates HEC token via auth service (with 5-min caching)
   - IP-based and token-based rate limiting (Redis-backed)
   - **Normalizes events to OCSF format**:
     - Registry pattern matches raw event format/source_type to normalizer
     - 77 auto-generated normalizers (one per OCSF class) in `ingest/internal/normalizer/generated/`
     - HECNormalizer as fallback for generic HEC events
     - Validation chain ensures OCSF compliance
     - Failed events → Dead Letter Queue (file-based at `/var/lib/telhawk/dlq`)
   - Forwards normalized events to storage service
   - Implements retry with exponential backoff (3 attempts, ~700ms total)
   - Supports HEC ack channel for event tracking

2. **Storage Service** persists to OpenSearch
   - Bulk indexing with automatic retry (3 attempts, exponential backoff)
   - Index pattern: `telhawk-events-YYYY.MM.DD`
   - OCSF-optimized field mappings

**Key Files:**
- `ingest/internal/pipeline/pipeline.go`: Orchestrates normalization and validation
- `ingest/internal/normalizer/normalizer.go`: Registry and interface definitions
- `ingest/internal/normalizer/generated/`: Auto-generated normalizers (77 OCSF classes)
- `ingest/internal/handlers/hec.go`: HEC endpoint implementation
- `storage/internal/client/opensearch.go`: OpenSearch bulk operations
- `common/ocsf/`: Shared OCSF event structures and types

### Authentication Flow

1. User login (`POST /api/v1/auth/login`) → returns JWT access token + refresh token
2. Access token used in `Authorization: Bearer <token>` header
3. Token validation endpoint (`POST /api/v1/auth/validate`) called by other services
4. Refresh tokens stored in PostgreSQL sessions table with revocation support
5. HEC tokens stored separately with user association
6. All auth events forwarded to ingest service as OCSF Authentication events (class_uid: 3002)

**Key Files:**
- `auth/internal/repository/postgres.go`: Database operations
- `auth/pkg/tokens/jwt.go`: JWT generation and validation
- `auth/migrations/001_init.up.sql`: Database schema

### API Conventions (JSON:API)

All new and modernized HTTP APIs follow the JSON:API 1.0 specification (https://jsonapi.org/format/):
- Content type: `application/vnd.api+json` for requests and responses
- Top-level members: `data`, `errors`, `meta`, `links`
- Resources: `type`, `id`, `attributes`, optional `relationships`
- Pagination: `page[number]`, `page[size]` (or cursor when specified)
- Filtering and sorting use JSON:API conventions (e.g., `filter[...]`, `sort`)
- Errors use JSON:API error objects with `status`, `code`, `title`, `detail`

Example resources include Saved Searches (`type: saved-search`), where immutable versioning is represented in attributes (e.g., `version_id`, lifecycle timestamps) and ownership is conveyed via `relationships` to `user`.

### OCSF Normalization

The system uses a code generator approach for OCSF compliance:

1. **OCSF Schema** (`ocsf-schema/`): Git submodule tracking OCSF 1.1.0 schema
2. **OCSF Type Generator** (`tools/ocsf-generator/`): Generates Go structs for OCSF events and objects
3. **Normalizer Generator** (`tools/normalizer-generator/`): Generates normalizers for each OCSF class
4. **Generated OCSF Types** (`common/ocsf/`): Shared OCSF event structures used across services
5. **Generated Normalizers** (`ingest/internal/normalizer/generated/`): One normalizer per OCSF class (77 total)
6. **Runtime**: Registry in ingest service matches events to normalizers based on source_type patterns

Each normalizer implements:
- Field mapping (common variants → OCSF standard fields)
- Event classification (category_uid, class_uid, activity_id, type_uid)
- Metadata enrichment (product info, timestamps, severity)

**Key Files:**
- `tools/ocsf-generator/main.go`: OCSF type generator
- `tools/normalizer-generator/main.go`: Normalizer code generator
- `common/ocsf/`: Shared OCSF event types and objects (generated)
- `ingest/internal/normalizer/registry.go`: Normalizer registration
- `ingest/internal/normalizer/generated/`: Auto-generated normalizers

### Configuration Management

All services follow a consistent pattern:
- **YAML config file** embedded in Docker images at `/etc/telhawk/<service>/config.yaml`
- **Environment variables** override YAML settings (12-factor app compliant)
- **Viper** library for config loading
- **No CLI arguments** for configuration

Environment variable naming: `<SERVICE>_<SECTION>_<KEY>`
Examples:
- `AUTH_SERVER_PORT=8080`
- `INGEST_AUTH_URL=http://auth:8080`
- `QUERY_OPENSEARCH_PASSWORD=secret`

**Key Files:**
- `auth/config.yaml`, `ingest/config.yaml`, etc.: Default configurations
- `docker-compose.yml`: Shows environment variable overrides
- `.env.example`: Template for local overrides

## TLS/Certificate Management

The stack uses mutual TLS (mTLS) for service-to-service communication:

1. **Certificate Generation**: Two init containers create certificates before services start
   - `telhawk-certs`: Generates certs for Go services (auth, ingest, core, storage, query, web)
   - `opensearch-certs`: Generates certs for OpenSearch

2. **Certificate Storage**: Shared Docker volumes
   - `telhawk-certs:/certs` - mounted read-only to all Go services
   - `opensearch-certs:/certs` - mounted read-only to OpenSearch

3. **TLS Configuration**: Controlled via environment variables
   - `<SERVICE>_TLS_ENABLED=true/false`: Enable TLS for each service
   - `<SERVICE>_TLS_SKIP_VERIFY=true/false`: Skip cert verification (dev only)

**Key Files:**
- `certs/generator/Dockerfile`: Go service certificate generator
- `opensearch/cert-generator/Dockerfile`: OpenSearch certificate generator
- `docs/TLS_CONFIGURATION.md`: Detailed TLS setup guide

## Important Implementation Details

### Database Schema Patterns

**PostgreSQL - Immutability Pattern**:
The system follows an immutable database pattern for audit trails and versioning:

**Core Principles**:
- **UUID v7**: All new IDs use `uuid.NewV7()` for time-ordered UUIDs (better B-tree performance than random UUIDs)
- **Lifecycle Timestamps**: Use `disabled_at`, `deleted_at`, `hidden_at` instead of boolean flags for audit trails
- **Append-Only Pattern**: INSERT for new content, UPDATE only for lifecycle timestamps
- **No Physical Deletes**: Soft delete with `deleted_at` timestamp and `deleted_by` user reference
- **Immutable Versioning**: Content changes create new rows with same stable ID but new version ID

**Auth Service (users, hec_tokens, sessions)**:
- UUIDs: UUID v7 primary keys
- Lifecycle: `created_at`, `disabled_at`, `disabled_by`, `deleted_at`, `deleted_by`
- No `enabled` boolean or `updated_at` timestamp
- Helper methods: `IsActive()` checks lifecycle state
- Example:
  ```sql
  CREATE TABLE users (
      id UUID PRIMARY KEY,
      username VARCHAR(255) NOT NULL UNIQUE,
      created_at TIMESTAMP NOT NULL DEFAULT NOW(),
      disabled_at TIMESTAMP,
      disabled_by UUID REFERENCES users(id),
      deleted_at TIMESTAMP,
      deleted_by UUID REFERENCES users(id)
  );
  ```

**Rules Service (detection_schemas)**:
- Immutable versioning: Same `id` (stable) + new `version_id` (version-specific) per update
- Window functions calculate version numbers on read (no race conditions)
- Lifecycle: `created_at`, `disabled_at`, `disabled_by`, `hidden_at`, `hidden_by`
- Server-generated UUIDs: Users NEVER provide `id` or `version_id` in requests
- POST creates new rule (server generates both IDs), PUT creates new version (reuses `id`, new `version_id`)
- Example:
  ```sql
  CREATE TABLE detection_schemas (
      id UUID NOT NULL,                 -- Stable identifier (groups versions)
      version_id UUID PRIMARY KEY,      -- Version-specific UUID (UUID v7)
      model JSONB NOT NULL,
      view JSONB NOT NULL,
      controller JSONB NOT NULL,
      created_at TIMESTAMP NOT NULL DEFAULT NOW(),
      disabled_at TIMESTAMP,
      disabled_by UUID,
      hidden_at TIMESTAMP,
      hidden_by UUID
  );
  ```

**Alerting Service (cases, case_alerts)**:
- Cases: UUID v7 IDs, mutable status field (open, in_progress, resolved, closed)
- Case lifecycle: `created_at`, `updated_at`, `closed_at`, `closed_by`
- Case-Alert junction: Links cases to alerts (OpenSearch documents)
- Foreign keys: `detection_schema_id` (stable) and `detection_schema_version_id` (version)

**OpenSearch**:
- Daily time-based indices: `telhawk-events-YYYY.MM.DD`, `telhawk-alerts-YYYY.MM.DD`
- OCSF-optimized mappings (nested objects for actors, devices, etc.)
- Retention managed via index lifecycle policies
- Query pattern: `telhawk-events-*` for searches across all indices

### Error Handling and Reliability

**Dead Letter Queue (DLQ)**:
- File-based storage at `/var/lib/telhawk/dlq`
- Captures normalization and storage failures
- Preserves full event context for debugging/replay
- API endpoints: `GET /dlq/list`, `POST /dlq/purge`
- Metrics exposed via health endpoint

**Retry Strategy**:
- Ingest → Core: 3 attempts, exponential backoff (~700ms total)
- Core → Storage: 3 attempts, exponential backoff
- Retries on 5xx, 429, network errors
- No retry on 4xx client errors (except 429)

**Rate Limiting**:
- Redis-backed sliding window algorithm
- IP-based (pre-auth) and token-based (post-auth)
- Returns HTTP 429 when exceeded
- Graceful degradation if Redis unavailable

### Observability

**Health Checks**:
- All services expose `/healthz` or `/readyz` endpoints

**Metrics** (Prometheus format at `/metrics`):
- Event processing: `events_total`, `normalization_duration`, `storage_duration`
- Queue depth: `queue_depth`, `queue_capacity`
- Rate limiting: `rate_limit_hits_total`
- Acks: `acks_pending`, `acks_completed_total`

### Running Tests

```bash
# Run all tests
go test ./...

# Run with verbose output
go test -v ./...

```

## CLI Tool (thawk)

The CLI uses Cobra for command structure:

```bash
# Authentication
thawk auth login -u <username> -p <password>
thawk auth whoami
thawk auth logout

# HEC token management
thawk token create --name <token-name>
thawk token list
thawk token revoke <token-id>

# Event ingestion
thawk ingest send --message "event text" --token <hec-token>

# Search queries
thawk search --query "severity:high" --from 1h
```

**Key Files:**
- `cli/cmd/root.go`: Root command and global flags
- `cli/cmd/auth.go`, `cli/cmd/token.go`, etc.: Subcommands
- `cli/internal/config/config.go`: CLI configuration (~/.thawk/config.yaml)

## Code Generation

### OCSF Normalizer Generator

Location: `tools/normalizer-generator/`

Regenerate normalizers after OCSF schema updates:

```bash
cd tools/normalizer-generator
go run main.go

# Output: core/internal/normalizer/generated/*.go (77 files)
```

The generator:
- Reads OCSF schema from `ocsf-schema/` directory
- Generates one normalizer per event class
- Creates intelligent field mappings
- Outputs registration code for the normalizer registry

## Common Development Patterns

### Adding a New Service

1. Create service directory with standard structure:
   ```
   newservice/
   ├── cmd/newservice/main.go
   ├── internal/
   │   ├── config/config.go
   │   ├── handlers/handlers.go
   │   └── ...
   ├── Dockerfile
   ├── config.yaml
   └── go.mod
   ```

2. Add to `docker-compose.yml` with health checks and dependencies
3. Add TLS certificate generation if needed
4. Follow environment variable naming: `NEWSERVICE_SECTION_KEY`

### Adding Database Migrations

1. Create numbered migration files in `auth/migrations/`:
   - `NNN_description.up.sql`
   - `NNN_description.down.sql`

2. Migrations run automatically on auth service startup
3. Use PostgreSQL best practices: indexes, constraints, comments

## Security Considerations

**Network Exposure:**
- **Minimal Attack Surface**: Only web (3000) and ingest (8088) exposed externally
- **No Direct Service Access**: Auth, query, core, and storage services ONLY accessible via Docker network
- **Self-Registration Disabled**: User accounts must be created by administrators (registration endpoint disabled)
- **Admin Access**: All administrative operations require authentication via web UI or thawk CLI

**Authentication & Authorization:**
- JWT secrets MUST be set via `AUTH_JWT_SECRET` environment variable
- HEC tokens are random UUIDs, stored hashed in database
- All auth events forwarded to SIEM for audit trail (nonrepudiation)
- Audit log table captures all authentication/authorization events with HMAC signatures

**Transport Security:**
- TLS MUST be enabled in production (`*_TLS_ENABLED=true`)
- PostgreSQL uses SSL/TLS in production (`sslmode=require`)
- OpenSearch uses TLS with client certificates

**Operational Security:**
- Default passwords in `docker-compose.yml` MUST be changed for production
- Rate limiting prevents abuse of ingestion endpoints
- Dead Letter Queue captures failed events for forensic analysis

## Documentation

Key documentation files:
- `README.md`: Overview, quick start, architecture
- `docs/CONFIGURATION.md`: Complete configuration reference
- `docs/CLI_CONFIGURATION.md`: CLI usage guide
- `DOCKER.md`: Docker commands and troubleshooting
- `TODO.md`: Development roadmap and recent accomplishments
- Individual service READMEs in service directories

## Web Frontend

The web UI is a React application with:
- **Backend**: Go server in `web/backend/` (port 3000)
- **Frontend**: React app in `web/frontend/`
- **Build**: Frontend built and served as static files by backend
- **Features**: Search console, event table, OCSF field inspection, severity color-coding

Frontend development:
```bash
cd web/frontend
npm install
npm start  # Development server
npm run build  # Production build
```

## Default Credentials

**Database (development)**:
- PostgreSQL: `telhawk:telhawk-auth-dev@auth-db:5432/telhawk_auth`
- OpenSearch: `admin:TelHawk123!`

**Default User (created by migration)**:
- Username: `admin`
- Password: `admin123`
- Email: `admin@telhawk.local`
- Roles: `[admin]`
